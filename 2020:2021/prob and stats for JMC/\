\documentclass[12pt]{report}
\usepackage[thinc]{esdiff} % for typesettign derivatives
\usepackage{amsthm} % provides an enhanced version of LaTex's \newtheorem command
\usepackage{mdframed} % framed environments that can split at page boundaries
\usepackage{enumitem} % bulletin points or other means of listing things
\usepackage{amssymb} % for AMS symbols
\usepackage{amsmath} % so as to use align
\usepackage{latexsym} % so as to use symbols like \leadsto
\usepackage{mathrsfs} % for using mathscr for char like operators
\usepackage{commath} % for using norm symbol
\usepackage{authblk} % for writing affiliations
\usepackage{graphicx} % for importing images
\graphicspath{{./images/}} % for the path to images, also always put label behind captions
\usepackage{textcomp} % for using degree symbol
\usepackage{hyperref} % for clickable link in the pdf & customizable reference text
\usepackage[all]{hypcap} % for clickable link to images instead of caption
\usepackage[margin=1in]{geometry} % default is 1.5in
% \usepackage[left=0.4in, right=0.4in, top=0.8in, bottom=0.8in]{geometry}
\usepackage[title]{appendix} % for attaching appendix
\allowdisplaybreaks % allow page breaking in display maths, like align
% allow for more advanced table layout
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
% for adjusting caption settings
\usepackage{caption}
\captionsetup[table]{skip=10pt}

\theoremstyle{definition}
\mdfdefinestyle{defEnv}{%
  hidealllines=false,
  nobreak=true,
  innertopmargin=-1ex,
}

% The following is for writing block of code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\setlength{\parindent}{0pt}
\setlength{\fboxrule}{2pt}

% Use the following to change code language and related settings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\pagestyle{headings}
\author{Lectured by Alex Geringer-Sameth}
\title{Probability and Statistics for JMC}
\affil{Typed by Aris Zhu Yi Qing}
\begin{document}
\maketitle
\tableofcontents

\chapter{Review of Elementary Set Theory}

\noindent\fbox{%
    \parbox{\textwidth}{%
        \begin{align*}
            \Omega & \qquad\text{universal set} \\
            \medskip
            \emptyset & \qquad\text{empty set} \\
            \medskip
            A\subseteq\Omega & \qquad\text{subset of $\Omega$} \\
            \medskip
            \overline{A} & \qquad\text{Complement of $A$} \\
            \medskip
            |A| & \qquad\text{cardinality of $A$} \\
            \medskip
            A\cup B & \qquad\text{union ($A$ or $B$)} \\
            \medskip
            A\cap B & \qquad\text{intersection($A$ and $B$)} \\
            \medskip
            A=B & \qquad\text{both sets have exactly the same elements} \\
            \medskip
            A\backslash B & \qquad\text{set difference (elements in $A$ that are
            not in $B$)} \\
            \medskip
            \left\{\omega\right\} & \qquad\text{a singleton with only the
            element $\omega$ in the set} \\
            \medskip
            A\times B & \qquad\left\{(a,b)|a\in A, b\in B\right\}
        \end{align*}  
    }%
}

\chapter{Visual and Numerical Summaries}

\section{Visualization}

\begin{figure}
  	\includegraphics[scale=0.4]{./images/histogram_empirical_CDF.jpeg}
  	\centering
    \caption{The first diagram is the histogram, and the second diagram is the
    empirical cdf with the same set of data}\label{hist_cdf}
\end{figure}


\newmdtheoremenv[style=defEnv]{theorem}{Definition}
\begin{theorem}
The \textbf{\emph{histogram}} allows us to visualize how a sample of data
is distributed, say the observed values are $\left\{x_1,\ldots,x_2\right\}$.
The first step is deciding on a set of \textbf{\emph{bins}} that divide the
range of $x$ into a series of intervals.
A histogram then shows the \textbf{\emph{frequency}} for each bin.
\end{theorem}
\paragraph{\underline{Comments}}
Often the histogram's $y$-axis is normalized in some way.
\begin{itemize}
    \item 
        Instead of showing frequency, the height of the histogram can show
        \textbf{\emph{relative frequency}}, the fraction of the data set contained within the
        bin. 
        In this case, $1=\sum_{\text{bins }i} y_i$, where $y_i$ is the relative
        frequency at bin $i$.
    \item 
        The histogram could also show the \textbf{\emph{density}}, the relative
        frequency divided by the bin width.
        In this case, $1=\sum_{\text{bins }i} \rho_i\Delta x_i$, where $\rho_i$
        is the density for bin $i$ and $\Delta x_i$ is the width of bin $i$.
\end{itemize} 

\newmdtheoremenv[style=defEnv]{empirical CDF}[theorem]{Definition}
\begin{empirical CDF}
    The \textbf{\emph{empirical cumulative distribution function}} of a sample of
    real values $\left\{x_1,\ldots,x_n\right\}$ is
    \[
        F_n(x)=\frac{1}{n}\sum_{i=1}^{n} I(x_i\le x),
    \]
    where $I(x_i\le x)$ is an \textbf{\emph{indicator function}}, i.e.\ the
    value is 1 when $x_i\le x$ and 0 when $x_i>x$.
\end{empirical CDF}

\section{Summary Statistics}

\subsection{Measures of Location}

\noindent\fbox{%
    \parbox{\textwidth}{%
        \begin{align*}
            \text{arithmetic mean} & \qquad \overline{x}=\frac{1}{n}\sum_{i=1}^{n} x_i \\
            \text{geometric mean} & \qquad x_G={\left(\prod_{i=1}^{n}
            x_i\right)}^{\frac{1}{n}} \\
            \text{harmonic mean} &  \qquad\frac{1}{x_H}=\frac{1}{n}\sum_{i=1}^{n}
            \frac{1}{x_i} \\
            \text{$i^{\text{th}}$ order statistic} & \qquad x_{(i)}= \text{the
            $i^{\text{th}}$ smallest value of the sample} \\
            \text{median} &\qquad x_{\left(\frac{n+1}{2}\right)} \\
            \text{mode} & \qquad \text{$x_i$ which occurs most
            frequently in the sample}
        \end{align*} 
    }%
}
\paragraph{\underline{Comments}}
\begin{itemize}
        \item For positive data $\left\{x_1,\ldots,x_n\right\}$,
            \[
                \text{arithmetic mean}\ge\text{geometric mean}\ge\text{harmonic
                mean}.
            \]
        \item Arithmetic mean and geometric mean are related in the following
            way:
            \[
                \overline{x} = \frac{1}{n}\sum_{i=1}^{n} x_i =
                \frac{1}{n}\sum_{i=1}^{n} \ln{y_i} =
                \frac{1}{n}\ln{\prod_{i=1}^{n} y_i} =
                \ln{{\left(\prod_{i=1}^{n} y_i\right)}^{\frac{1}{n}}} =
                \ln{x_G},
            \]
            where $x_i = \ln{y_i}$.
        \item
            For $x_{(i)}$, when $i$ is not an integer, we define
            $\alpha\in(0,1)$ s.t. $\alpha=i-\lfloor i\rfloor $, and
            \[
                x_{(i)}=(1-\alpha)x_{(\lfloor i\rfloor)} 
                + \alpha x_{(\lceil i\rceil )}.
            \]
\end{itemize} 

\subsection{Measures of Dispersion}

\noindent\fbox{%
    \parbox{\textwidth}{%
        \begin{align*}
            \text{mean square/sample variance} & \qquad
            s^{2}=\frac{1}{n}\sum_{i=1}^{n} {(x_i-\overline{x})}^{2} \\
            \text{root mean square/sample standard deviation} & \qquad
            s = \sqrt{s^{2}} \\
            \text{range} & \qquad x_{(n)}-x_{(1)} \\
            \text{first quartile} & \qquad x_{\left(\frac{1}{4}(n+1)\right)} \\
            \text{third quartile} & \qquad x_{\left(\frac{3}{4}(n+1)\right)} \\
            \text{interquartile range} & \qquad 
            x_{\left(\frac{1}{4}(n+1)\right)} - x_{\left(\frac{3}{4}(n+1)\right)}
        \end{align*} 
    }%
}
\paragraph{Comments}
\begin{itemize}
        \item sample variance's different expression:
            \[
                s^{2} = \frac{1}{n}\sum_{i=1}^{n} {(x_i-\overline{x})}^{2}
                = \left(\frac{1}{n}\sum_{i=1}^{n} x_i^2\right)
                - {\left(\frac{1}{n}\sum_{i=1}^{n} x_i\right)}^{2}
                = \overline{x^{2}}-\overline{x}^{2}.
            \]
        \item Robustness, shown in table~\ref{robustness}
            \begin{table}[h]
                \centering
                \caption{Robustness of different location and dispersion statistic}
                \label{robustness}
                \begin{tabular}{l||c|c|c}
                    & Least Robust & More Robust & Most Robust \\
                    \hline\hline
                    Location & $\frac{x_{(1)}+x_{(n)}}{2}$ & $\overline{x}$ &
                    $x_{\left(\frac{n+1}{2}\right)}$ \\
                    \hline
                    Dispersion & $x_{(n)}-x_{(1)}$ & $s$ &
                    $x_{\left(\frac{3}{4}(n+1)\right)}-x_{\left(\frac{1}{4}(n+1)\right)}$
                \end{tabular} 
            \end{table} 
\end{itemize} 

\subsection{Covariance, Correlation, and Skewness}

\noindent\fbox{%
    \parbox{\textwidth}{%
        \begin{align*}
            \text{covariance} & \qquad s_{xy}=\frac{1}{n}\sum_{i=1}^{n}
            (x_i-\overline{x})(y_i-\overline{y}) \\
            \text{correlation} & \qquad r_{xy}=\frac{s_{xy}}{s_xs_y} \\
            \text{skewness} & \qquad \frac{1}{n}\sum_{i=1}^{n} 
            {\left(\frac{x_i-\overline{x}}{s}\right)}^{3}
        \end{align*} 
    }%
}
\paragraph{Comments}
\begin{itemize}
        \item covariance's different expression:
            \[
                s_{xy}=\frac{1}{n}\sum_{i=1}^{n}
                (x_i-\overline{x})(y_i-\overline{y})
                = \frac{1}{n}\sum_{i=1}^{n} x_iy_i+
                \frac{1}{n}\sum_{i=1}^{n} -x_i \overline{y}
                -\overline{x}y_i+\overline{x}\overline{y}
                = \frac{\sum_{i=1}^{n} x_iy_i}{n}
                -\overline{x}\,\overline{y}.
            \]
            In the random variable's context, it is
            \[
                \text{Cov}(X,Y)=E(XY)-E(X)E(Y).
            \]
        \item Correlation gives a \textbf{scale-invariant}
            measurement of relatedness between $x$ and $y$, since
            \[
                |r_{xy}|\le 1.
            \]
        \item A sample is \textbf{positively (negatively)} or
            \textbf{right (left) skewed} if the upper tail of the histogram of
            the sample is longer (shorter) than the lower tail.
\end{itemize} 

\subsection{Box-and-whisker plot}

The diagram is based on the five-point summary (use Figure~\ref{box-plot} as
reference):
\begin{itemize}
    \item Median -- middle line in the box.
    \item 3\textsuperscript{rd} and 1\textsuperscript{st} Quartiles -- top and
        bottom of the box.
    \item ``Whiskers'' -- extend out as dashed lines from the box to max/min
        values, which are the two short horizontal lines.
    \item Any outliers, i.e.\ extreme points beyond the whiskers, are plotted
        individually as dots.
\end{itemize} 
\begin{figure}[h]
  	\includegraphics[scale=0.18]{./images/box-and-whisker.jpeg}
  	\centering
  	\caption{the counts of insects found in agricultural experimental units
    treated with six different insecticides A-F}\label{box-plot}
\end{figure}

\chapter{Probability}

\section{Formal Definition of Probability}

\subsection{$\sigma$-algebra}

\newmdtheoremenv[style=defEnv]{sigma algebra}[theorem]{Definition}
\begin{sigma algebra}\label{sigma-algebra}
    $\mathcal{F}$, a collection of subsets of a set $S$,
    is called a \textbf{\emph{$\sigma$-algebra}} associated with
    $S$ if:
    \begin{enumerate}[label = (\alph*)]
        \item $S\in\mathcal{F}$,
        \item $\mathcal{F}$ is closed under complements w.r.t. $S$:
            \[
                E\in\mathcal{F} \Longrightarrow \overline{E}\in\mathcal{F},
            \]
        \item $\mathcal{F}$ is closed under countable unions:
            \[
                E_1,E_2,\ldots\in\mathcal{F}\Longrightarrow
                \bigcup_{i=1}^{\infty}E_i\in\mathcal{F}.
            \]
    \end{enumerate} 
\end{sigma algebra}

\paragraph{Comments}
Definition~\ref{sigma-algebra} implies two facts.
\begin{enumerate}
    \item $\mathcal{F}$ must contain the empty set $\emptyset$.
        \begin{proof}
            Since $S\in\mathcal{F}$, we have
            $\overline{S}=\emptyset\in\mathcal{F}$.
        \end{proof} 
    \item $\mathcal{F}$ must be closed under countable intersections.
        \begin{proof}
            Let $E_1,E_2,\ldots\in\mathcal{F}$. We can then imply the following:
            \[
                \overline{E_1},\overline{E_2},\ldots\in\mathcal{F}
                \Rightarrow \bigcup_i \overline{E_i}\in\mathcal{F}
                \Rightarrow \overline{\bigcup_i \overline{E_i}}\in\mathcal{F}
                \xrightarrow{\text{De Morgan's Law}}\bigcap_iE_i\in\mathcal{F}.
            \]
        \end{proof} 
\end{enumerate} 
In short, we can take unions, intersections, and complements of members of
$\mathcal{F}$ in any combination and the result will always be a member of
$\mathcal{F}$.

\subsection{Probability Measure}

\newmdtheoremenv[style=defEnv]{probability measure and space}[theorem]
{(Kolmogorov's axioms of probability) Definition}
\begin{probability measure and space}
    A \textbf{\emph{probability measure}} $P$ is a function
    $P:\mathcal{F}\mapsto\mathbb{R}$ satisfying
    \begin{enumerate}[label = (\alph*)]
        \item $P(E)\ge 0 \;\forall E\in\mathcal{F}$,
        \item $P(S)=1$,
        \item If $E_1,E_2,\ldots\in\mathcal{F}$ are disjoint (i.e.\ $E_i\cap
            E_j=\emptyset \;\forall i\neq j$) then
            \[
                P\left(\bigcup_{i=1}^{\infty}E_i\right)=
                \sum_{i=1}^{\infty} P(E_i).
            \]
    \end{enumerate} 
    The triplet $(S,\mathcal{F},P)$, consisting of a set $S$, a $\sigma$-algebra
    $\mathcal{F}$ of subsets of $S$, and a probability measure $P$, is called a
    \textbf{\emph{probability space}}.
\end{probability measure and space}

\paragraph{Comments}
\begin{itemize}
    \item The \textbf{\emph{sample space}} $(S)$ is the set of all possible
        outcomes of an experiment.
    \item The \textbf{\emph{event space}} $(\mathcal{F})$ is the set of possible
        events, where an \textbf{\emph{event}} $E$ is a subset of the sample
        space, $E\subseteq S$. An \textbf{\emph{elementary event}} is one that
        consist of a single element of $S$, i.e.\ a singleton.
    \item The probability measure $(P)$ has three important interpretations:
        \begin{enumerate}
            \item \textbf{classical}:
                Different outcomes in the sample space $S$ 
                are ``equally likely'',
            \item \textbf{frequentist}:
                the relative frequency of an event over many trials,
            \item \textbf{subjective}:
                a numerical measure of the degree of belief held by an individual.
        \end{enumerate} 
        \newtheorem{probability-interpretations}[theorem]{Example}
        \begin{probability-interpretations}
            ``A sensor can detect items within 10 cm of the sensor.
            The sensor is placed in a room together with an object, and
            the probability that the sensor makes a detection is 0.0001.''
            \begin{enumerate}
                \item \textbf{classical}:
                    The volume within 10 cm of the sensor divided by the volume
                    of the room is 0.0001.
                \item \textbf{frequentist}:
                    If we repeat the experiment a lot of times, then the
                    fraction of the experiments in which the sensor makes a
                    detection is 0.0001.
                \item \textbf{subjective}:
                    Someone's subjective degree of belief, measured on a
                    numerical scale from 0 to 1, that the sensor will detect is
                    0.0001.
            \end{enumerate} 
        \end{probability-interpretations}
    \item several results that can be derived from the probability measure
        axioms:
        \begin{itemize}
            \item $P(\emptyset)=0$.
            \item $P(E)\le 1$.
            \item $P(\overline{E})=1-P(E)$.
            \item $P(E\cup F)=P(E)+P(F)-P(E\cap F)$..
            \item $P(E\cap \overline{F})=P(E)-P(E\cap F)$.
            \item If $E\subset F$ then $P(E)\le P(F)$.
        \end{itemize} 
\end{itemize} 

\section{Conditional Probability}

\newmdtheoremenv[style=defEnv]{conditional probability}[theorem]{Definition}
\begin{conditional probability}
    If $P(F)>0$ then the \textbf{\emph{conditional probability}} of $E$ given
    $F$ is
    \[
        P(E|F)=\frac{P(E\cap F)}{P(F)}.
    \]
\end{conditional probability}
\paragraph{Comments}
\begin{itemize}
    \item Difference among the following forms:
        \begin{itemize}
            \item $P(E|F)$ -- \textbf{\emph{conditional probabilities}},
            \item $P(E\cap F)$ -- \textbf{\emph{joint probabilities}},
            \item $P(E)$ -- \textbf{\emph{marginal probabilities}}.
        \end{itemize} 
    \item several results derived from the conditional probability definition:
        \begin{itemize}
            \item $P(E|F)\ge 0$ for any event $E$.
            \item $P(F|F)=1$.
            \item If the events $E_1,E_1,\ldots$ are pairwise disjoint, then
                $P\left(\left(\bigcup_i E_i\right)|F\right)=\sum_{i} P(E_i|F)$.
        \end{itemize} 
    \item \underline{Warning}: In general, $P(E|F)\neq P(F|E)$.
\end{itemize} 

\newtheorem{conditional probability eg}[theorem]{Example}
\begin{conditional probability eg}
    A medical test for a disease $D$ has outcomes $+$ and $-$.
    The probabilities are
    \begin{table}[h]
        \centering
        \begin{tabular}{l||c|c|c}
            & $D$ & $\overline{D}$ & \\
            \hline\hline
            $+$ & 0.009 & 0.099 & 0.108 \\
            \hline
            $-$ & 0.001 & 0.891 & 0.892 \\
            \hline
                & 0.01 & 0.99 &
        \end{tabular} 
    \end{table} 

    By the definition of conditional probability, we have
    \[
        P(+|D)=90\%,\quad P(-|\overline{D})=90\%, \quad
        P(D|+)=\frac{0.009}{0.108}\approx 0.083.
    \]
    The first two probabilities show that the test is fairly accurate. Sick
    people yield a positive 90\% of the time and healthy people yield a negative
    90\% of the time.
\end{conditional probability eg}


\section{Independence}

\newmdtheoremenv[style=defEnv]{independence}[theorem]{Definition}
\begin{independence}
    Two events $E$ and $F$ are \textbf{\emph{independent}} iff
    \[
        P(E\cap F)=P(E)P(F).
    \]
\end{independence}
\paragraph{Comments}
\begin{itemize}
    \item Extension: The events $E_1,\ldots,E_k$ are independent if, for every
        subset of events of size $l\le k$, say indexed by
        $\left\{i_1,\ldots,i_l\right\}$,
        \[
            P\left(\bigcap_{j=1}^l E_{i_j}\right)=\prod_{j=1}^{l} P(E_{i_j}).
        \]
    \item Independence could be either assumed or verified via the definition.
    \item Disjoint events with positive probability are not independent.
    \item From the definition of conditional probability, we can deduce that
        $E$ and $F$ are independent iff $P(E|F)=P(E)$.
\end{itemize} 

\newmdtheoremenv[style=defEnv]{conditional independence}[theorem]{Definition}
\begin{conditional independence}
    For three events $E_1, E_2, F$, the pair of events $E_1$ and $E_2$ are
    said to be \textbf{\emph{conditionally independent given $F$}} iff
    \[
        P(E_1\cap E_2|F) = P(E_1|F)P(E_2|F).
    \]
    which could also be written as $E_1\bot E_2|F$.
\end{conditional independence}

\section{Bayes' Theorem}

\newmdtheoremenv[style=defEnv]{bayes' theorem}[theorem]{(Bayes' Theorem) Theorem}
\begin{bayes' theorem}
    If $P(F)>0$ and $P(E)>0$, we have
    \[
        P(E|F)=\frac{P(F|E)P(E)}{P(F)}.
    \]
\end{bayes' theorem}

\newmdtheoremenv[style=defEnv]{law of total probability}[theorem]{(The Law of
Total Probability) Theorem}
\begin{law of total probability}
    Let $E_1,E_2,\ldots$ be a partition of $S$. Then, for any event $F\subseteq
    S$, we have
    \[
        P(F)=\sum_{i}P(F|E_i)P(E_i).
    \] 
\end{law of total probability}




\end{document}
