\documentclass[12pt]{report}
\usepackage[thinc]{esdiff} % for typesettign derivatives
\usepackage{amsthm} % provides an enhanced version of LaTex's \newtheorem command
\usepackage{mdframed} % framed environments that can split at page boundaries
\usepackage{enumitem} % bulletin points or other means of listing things
\usepackage{amssymb} % for AMS symbols
\usepackage{amsmath} % so as to use align
\usepackage{latexsym} % so as to use symbols like \leadsto
\usepackage{mathrsfs} % for using mathscr for char like operators
\usepackage{commath} % for using norm symbol
\usepackage{mathtools} % for using environments like dcases
\usepackage{authblk} % for writing affiliations
\usepackage{graphicx} % for importing images
\graphicspath{{./images/}} % for the path to images, also always put label behind captions
\usepackage{textcomp} % for using degree symbol
\usepackage{hyperref} % for clickable link in the pdf & customizable reference text
\usepackage[all]{hypcap} % for clickable link to images instead of caption
\usepackage[margin=1.0in]{geometry} % default is 1.5in
% \usepackage[left=0.4in, right=0.4in, top=0.8in, bottom=0.8in]{geometry}
\usepackage[title]{appendix} % for attaching appendix
\allowdisplaybreaks % allow page breaking in display maths, like align
\usepackage{xcolor} % for setting color of a block of text, use \textcolor{<color>}{}
\usepackage[normalem]{ulem} % for strikethrough text, use \sout{}
% allow for more advanced table layout
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
% for adjusting caption settings
\usepackage{caption}
\captionsetup[table]{skip=10pt}

\theoremstyle{definition}
\mdfdefinestyle{defEnv}{%
  hidealllines=false,
  nobreak=true,
  innertopmargin=-1ex,
}

% The following is for writing block of code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% setting of the thickness of the 4 lines of box
\setlength{\fboxrule}{2pt}

% Use the following to change code language and related settings
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate={~} {$\sim$}{1}
}

\pagestyle{headings}
\author{Lectured by Dr Dante Kalise}
\title{Optimization}
\affil{Typed by Aris Zhu Yi Qing}
\begin{document}
\maketitle
\tableofcontents

\chapter{Mathematical Preliminaries}

\section{Topological Concepts}

\newmdtheoremenv[style=defEnv]{theorem}{Definition}
\begin{theorem}
    The \textbf{open ball} with center $c\in\mathbb{R}^{n}$ and radius $r$ is
    \[
        B(c,r)=\left\{\mathbf{x}:\norm{\mathbf{x}-c}<r\right\}.
    \]
    Similarly, the \textbf{closed ball} with center $c$ and radius $r$ is
    \[
        B[c,r]=\left\{\mathbf{x}:\norm{\mathbf{x}-c}\le r\right\}.
    \]
\end{theorem}

\newmdtheoremenv[style=defEnv]{interior point}[theorem]{Definition}
\begin{interior point}
    Given a set $U\subseteq\mathbb{R}^{n}$, a point $\mathbf{c}\in U$ is called an
    \textbf{interior point} of $U$ if $\exists r>0$ for which
    $B(\mathbf{c},r)\subseteq U$. The set of all interior points of a given set
    $U$ is called the interior of the set and is denoted by
    \[
        \text{int}(U)=\left\{\mathbf{x}\in U:B(\mathbf{x},r)\subseteq U\text{
        for some }r>0\right\}.
    \]
\end{interior point}

\newmdtheoremenv[style=defEnv]{boundary points}[theorem]{Definition}
\begin{boundary points}
    Given a set $U\subseteq\mathbb{R}^{n}$, a \textbf{boundary point} of $U$ is
    a vector $\mathbf{x}\in\mathbb{R}^{n}$ satisfying that any neighbourhood of $\mathbf{x}$
    contains at least one point in $U$ and at least one point in its
    completement $U^{c}$. 
    We denote
    \[
        \text{bd}(U) = \text{The set of all boundary points of a set $U$}.
    \]
\end{boundary points}

\newmdtheoremenv[style=defEnv]{closure}[theorem]{Definition}
\begin{closure}
    The \textbf{closure} of a set $U\subseteq\mathbb{R}^{n}$ is 
    the smallest closed set containing $U$,
    denoted by cl$(U)$ with
    \[
        \text{cl}(U)=U\cup\text{bd}(U).
    \]
\end{closure}

\newmdtheoremenv[style=defEnv]{boundedness}[theorem]{Definition}
\begin{boundedness}
    A set $U\subseteq\mathbb{R}^{n}$ is called \textbf{bounded}
    if $\exists M>0$ for which $U\subseteq B(0,M)$.
\end{boundedness}

\newmdtheoremenv[style=defEnv]{compactness}[theorem]{Definition}
\begin{compactness}
    A set $U\subseteq\mathbb{R}^{n}$ is called \textbf{compact}
    if it is closed and bounded.
\end{compactness}

\section{Multi-variable Calculus}

\newmdtheoremenv[style=defEnv]{directional derivative}[theorem]{Definition}
\begin{directional derivative}
    The \textbf{directional derivative} of a scalar function $f$ w.r.t.
    $\mathbf{d}$ at a point $\mathbf{x}$ is denoted as
    \[
        f'(\mathbf{x};\mathbf{d})=\nabla f(\mathbf{x})^T\mathbf{d}
    \]
\end{directional derivative}


\newmdtheoremenv[style=defEnv]{gradient and hessian of quadratic functions}[theorem]{Theorem}
\begin{gradient and hessian of quadratic functions}
    Given the general quadratic functions of the form
    \[
        f(\mathbf{w})=\mathbf{w}^TA\mathbf{w}+\mathbf{b}^T\mathbf{w}+\gamma
    \]
    we have
    \[
        \nabla f(\mathbf{w})=(A^T+A)\mathbf{w}+\mathbf{b},
        \qquad
        \nabla^2 f(\mathbf{w})=A+A^T.
    \]
    If $A$ is symmetric, then
    \[
        \nabla f(\mathbf{w})=2A\mathbf{w}+\mathbf{b},
        \qquad
        \nabla^2 f(\mathbf{w})=2A.
    \]
\end{gradient and hessian of quadratic functions}

\section{Positive Definiteness of Matrix}

\newmdtheoremenv[style=defEnv]{diagonal elem positive}[theorem]{Proposition}
\begin{diagonal elem positive}
    Let $A$ be a positive definite (semidefinite) matrix, then 
    \begin{itemize}
        \item the diagonal elements of $A$ are positive (nonnegative)
        \item Tr($A$) and det($A$) are positive (nonnegative)
    \end{itemize} 
\end{diagonal elem positive}

\newmdtheoremenv[style=defEnv]{eigenvalue characterization}[theorem]{(Test 1) Theorem}
\begin{eigenvalue characterization}
    Let $A\in\mathbb{R}^{n\times n}$ be symmetric, then
    \begin{itemize}
        \item $A$ is positive definite (semidefinite) iff all its eigenvalues
            are positive (nonnegative).
        \item $A$ is indefinte iff it has at least one positive eigenvalue and
            at least one negative eigenvalue.
    \end{itemize} 
\end{eigenvalue characterization}

\newmdtheoremenv[style=defEnv]{diagonal dominance}[theorem]{Definition}
\begin{diagonal dominance}
    Let $A\in\mathbb{R}^{n\times n}$ be symmetric, then
    \begin{itemize}
        \item $A$ is \textbf{diagonally dominant} if
            \[
                |A_{ii}| \ge \sum_{j\neq i}|A_{ij}|\quad\forall i=1,2,\ldots,n
            \]
        \item $A$ is \textbf{strictly diagonally dominant} if
            \[
                |A_{ii}|>\sum_{j\neq i}|A_{ij}|\quad\forall i=1,2,\ldots,n
            \]
    \end{itemize} 
\end{diagonal dominance}

\newmdtheoremenv[style=defEnv]{positive definiteness of diagonally dominant matrices}[theorem]{(Test 2) Theorem}
\begin{positive definiteness of diagonally dominant matrices}
    If $A\in\mathbb{R}^{n\times n}$ is symmetric, diagonally dominant with
    positive (nonnegative) diagonal elements, then $A$ is positive definite
    (semidefinite).
\end{positive definiteness of diagonally dominant matrices}





\chapter{Unconstrained Optimization}

\section{Optimums}

\newmdtheoremenv[style=defEnv]{global optimum}[theorem]{Definition}
\begin{global optimum}
    Let $f:S\rightarrow\mathbb{R}$ be defined on a set
    $S\subseteq\mathbb{R}^{n}$, then $\forall \mathbf{x}\in S$,
    \[
        \mathbf{x}^* \in S \text{ is a \textbf{global minimum} point of $f$ over
        $S$ if $f(\mathbf{x})\ge f(\mathbf{x}^*)$},
    \]
    \[
        \mathbf{x}^* \in S \text{ is a \textbf{strict global minimum} point of $f$ over
        $S$ if $f(\mathbf{x})> f(\mathbf{x}^*)$},
    \]
    and similar definitions for maximum.
\end{global optimum}

\newmdtheoremenv[style=defEnv]{local optimum}[theorem]{Definition}
\begin{local optimum}
    Let $f:S\rightarrow\mathbb{R}$ be defined on a set
    $S\subseteq\mathbb{R}^{n}$, $\mathbf{x}^*\in S$ 
    is a \textbf{local minimum} of $f$ over $S$ if
    $\exists r>0$ s.t. $f(\mathbf{x}^*)\le f(\mathbf{x})$ 
    for any $\mathbf{x}\in S\cap B(\mathbf{x}^*,r)$.
    Similar definitions for \textbf{strict local minimum} and maximum.
\end{local optimum}

\newmdtheoremenv[style=defEnv]{stationary points}[theorem]{Definition}
\begin{stationary points}
    Let $f:U\rightarrow\mathbb{R}$ be a function defined on a set
    $U\subseteq\mathbb{R}^{n}$.
    Suppose that $\mathbf{x}^*\in\text{int}(U)$ and that all the partial
    derivatives of $f$ are defined at $\mathbf{x}^*$, then $\mathbf{x}^*$ is
    called a \textbf{stationary point} of $f$ if $\nabla f(\mathbf{x}^*)=0$.
\end{stationary points}

\section{Second-order Optimality Conditions}

\newmdtheoremenv[style=defEnv]{second-order optimality conditions}[theorem]{Theorem}
\begin{second-order optimality conditions}
    Let $f:U\rightarrow\mathbb{R}$ be a function defined on an open set
    $U\subseteq\mathbb{R}^{n}$. Suppose that $f$ is twice continuously
    differentiable over $U$ and that $\mathbf{x}^*$ is a stationary point, then
    \begin{itemize}
        \item $\mathbf{x}^*$ is a local minimum point $\iff$
            $\nabla^2f(\mathbf{x}^*)\succeq 0$.
        \item $\mathbf{x}^*$ is a strict local minimum point $\iff$
            $\nabla^2f(\mathbf{x}^*)\succ 0$.
        \item similar necessary and sufficient conditions for (strict) local
            maximum point
    \end{itemize} 
\end{second-order optimality conditions}

\newmdtheoremenv[style=defEnv]{saddle points}[theorem]{Definition}
\begin{saddle points}
    Let $f:U\rightarrow\mathbb{R}$ be a continuously differentiable function
    defined on an open set $U\subseteq\mathbb{R}^{n}$. A stationary point 
    $\mathbf{x}^*\in U$ is called a \textbf{saddle point} of $f$ over $U$ if it
    is neither a local minimum nor a local maximum point of $f$ over $U$.
\end{saddle points}

\newmdtheoremenv[style=defEnv]{sufficient condition for saddle points}[theorem]{Theorem}
\begin{sufficient condition for saddle points}
    Let $f:U\rightarrow\mathbb{R}$ be a continuously differentiable function
    defined on an open set $U\subseteq\mathbb{R}^{n}$. 
    Suppose that $f$ is twice continuously differentiable over $U$ and that
    $\mathbf{x}^*$ is a stationary point. Then
    \[
        \nabla^2 f(\mathbf{x}^*) \text{ is an indefinite matrix} \Longrightarrow \mathbf{x}^*
        \text{ is a saddle point of $f$ over $U$}.
    \]
\end{sufficient condition for saddle points}

\section{Attainment of Minimal/Maximal Points}

\newmdtheoremenv[style=defEnv]{Weierstrass' Theorem}[theorem]{(Weierstrass') Theorem}
\begin{Weierstrass' Theorem}
    Let $f$ be a continuous function defined over a nonempty conpact set
    $C\subseteq\mathbb{R}^{n}$. Then $\exists$ a global minimum point of $f$
    over $C$ and a global maximum point of $f$ over $C$.
\end{Weierstrass' Theorem}

\newmdtheoremenv[style=defEnv]{coerciveness}[theorem]{Definition}
\begin{coerciveness}
    Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a continuous function over
    $\mathbb{R}^{n}$. $f$ is called \textbf{coercive} if
    \[
        \underset{\norm{\mathbf{x}}\rightarrow\infty}{\lim}f(\mathbf{x})=\infty
    \]
\end{coerciveness}

\newmdtheoremenv[style=defEnv]{attainment of global optima}[theorem]{Theorem}
\begin{attainment of global optima}
    Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a continuous and coercive function
    and let $S\subseteq\mathbb{R}^{n}$ be a nonempty closed set. Then $f$
    attains a global minimum point on $S$.
\end{attainment of global optima}


\section{Global Optimality Conditions}

\newmdtheoremenv[style=defEnv]{global optimality condition}[theorem]{Theorem}
\begin{global optimality condition}
    Let $f$ be a twice continuously differentiable function defined over
    $\mathbb{R}^{n}$.
    Let $\mathbf{x}^*\in\mathbb{R}^{n}$ be a
    stationary point of $f$. Then
    \[
        \nabla^2 f(\mathbf{x})\succeq 0\;\forall \mathbf{x}\in\mathbb{R}^{n}
        \Longrightarrow
        \mathbf{x}^* \text{ is a global minimum point of $f$}.
    \]
\end{global optimality condition}

\newmdtheoremenv[style=defEnv]{quadratic function optimality}[theorem]{Proposition}
\begin{quadratic function optimality}
    Let $f(\mathbf{x})=\mathbf{x}^TA\mathbf{x}+2\mathbf{b}^T\mathbf{x}+c$, with
    $A\in\mathbb{R}^{n\times n}$ symmetric, then
    \begin{enumerate}
        \item $\mathbf{x}$ is a stationary point of $f$ iff
            $A\mathbf{x}=-\mathbf{b}$.
        \item if $A\succeq 0$, then $\mathbf{x}$ is a global minimum point of
            $f$ iff $A\mathbf{x}=-\mathbf{b}$.
        \item if $A\succ 0$, then $\mathbf{x}=-A^{-1}\mathbf{b}$ is a strict
            global minimum point of $f$.
    \end{enumerate} 
\end{quadratic function optimality}


\chapter{Linear Least Squares}

\section{Problem Formulation}

Consider the linear system
\[
    S\mathbf{x}\approx\mathbf{b},\quad(S\in\mathbb{R}^{m\times
    n},\mathbf{b}\in\mathbb{R}^{m},m>n)
\]
To solve the above system, the usual approach is to transform it to become
\[
    \underset{\mathbf{x}}{\text{min}}\norm{S\mathbf{x}-\mathbf{b}}^2
    \iff
    \underset{\mathbf{x}\in\mathbb{R}^{n}}{\text{min}}
    \left\{f(\mathbf{x})\equiv
    \mathbf{x}^TS^TS\mathbf{x}-2\mathbf{b}^TS\mathbf{x}+\norm{\mathbf{b}}^2\right\}.
\]
Note that $\nabla^2f(\mathbf{x})=2S^TS\succeq 0$ since
$\mathbf{x}^TS^TS\mathbf{x}=(S\mathbf{x})^T(S\mathbf{x})=\norm{S\mathbf{x}}^2\ge 0$.
Therefore, the unique optimal solution $\mathbf{x}_\text{LS}$ is the solution
$\nabla f(\mathbf{x})=0$, namely
\[
    (S^TS)\mathbf{x}_\text{LS}=S^T\mathbf{b} \Longrightarrow
    \mathbf{x}_\text{LS}={(S^TS)}^{-1}S^T\mathbf{b}.
\]

\section{Data Fitting}

\begin{enumerate}
    \item For dataset $(\mathbf{s}_i,b_i)$ where $\mathbf{s}_i\in\mathbb{R}^{n}$ 
        and $b_i\in\mathbb{R}$, we could transform to problem
        \[
            \underset{\mathbf{x}}{\text{min}}
            \sum_{i=1}^{m} {(\mathbf{s}_i^T\mathbf{x}-b_i)}^{2}
            \Longrightarrow
            \underset{\mathbf{x}}{\text{min}}
            \norm{S\mathbf{x}-\mathbf{b}}^2
        \]
    \item For polynomial fitting, given a set of points $\mathbb{R}^{2}:(u_i,y_i)$,
        the associated linear system is
        \[
            \begin{pmatrix}
                1 & u_1 & u_1^2 & \cdots & u_1^d \\
                1 & u_2 & u_2^2 & \cdots & u_2^d \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                1 & u_m & u_m^2 & \cdots & u_m^d \\
            \end{pmatrix} 
            \begin{pmatrix}
                a_0 \\
                a_1 \\
                \vdots \\
                a_d
            \end{pmatrix} 
            =
            \begin{pmatrix}
                y_0 \\
                y_1 \\
                \vdots \\
                y_m
            \end{pmatrix} 
        \]
\end{enumerate} 

\section{Regularized Least Squares}

A Regularized Least Square problem is formulated as
\[
    \underset{\mathbf{x}}{\text{min}}
    \norm{S\mathbf{x}-\mathbf{b}}^2
    +\lambda R(\mathbf{x}),
\]
where $\lambda$ is the regularization parameter and $R(\cdot)$ is the
regularization function (also called a \emph{penalty} function).
A common choice is a quadratic regularization function:
\[
    \underset{\mathbf{x}}{\text{min}}
    \norm{S\mathbf{x}-\mathbf{b}}^2
    +\lambda \norm{D\mathbf{x}}^2
\]
with its optimal solution being
\[
    \mathbf{x}_\text{RLS}={(S^TS+\lambda D^TD)}^{-1}S^T \mathbf{b}
\]
since $\nabla f=2S^TS\mathbf{x}-2S^T \mathbf{b}+2\lambda D^TD\mathbf{x}=0$.

\section{Denoising}

Suppose a noisy measurement of a signal $\mathbf{x}\in\mathbb{R}^{n}$ is given
\[
    \mathbf{b}=\mathbf{x}+\mathbf{w}
\]
where $\mathbf{x}$ is the ``true'' unknown signal, $\mathbf{w}$ is the unknown
noise and $\mathbf{b}$ is the (known) measures vector.
We could define
\[
    R(\mathbf{x})=\norm{L\mathbf{x}}^2, \text{ where }
    L=
    \begin{pmatrix}
        1 & -1 & 0 & 0 & \cdots & 0 & 0 \\
        0 & 1 & -1 & 0 & \cdots & 0 & 0 \\
        0 & 0 & 1 & -1 & \cdots & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 1 & -1
    \end{pmatrix} 
\]
as the regularization function to penalize any sudden variations in signal. The
RLS is thus
\[
    \underset{\mathbf{x}}{\text{min}}
    \norm{\mathbf{x}-\mathbf{b}}^2
    +\lambda\norm{L\mathbf{x}}^2
\]
with its direct solution being
\[
    \mathbf{x}_\text{RLS}(\lambda)={(I+\lambda L^TL)}^{-1}\mathbf{b}.
\]


\chapter{The Gradient Method}

\section{Descent Direction}

\newmdtheoremenv[style=defEnv]{descent direction}[theorem]{Definition}
\begin{descent direction}
    Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a continuously differentiable function.
    A vector $\mathbf{0}\neq\mathbf{d}\in\mathbb{R}^{n}$ is called a
    \textbf{descent direction} of $f$ at $\mathbf{x}$ if
    \[
        f'(\mathbf{x};\mathbf{d})=\nabla f(\mathbf{x})^T\mathbf{d}<0.
    \]
\end{descent direction}

\newtheorem{descent direction example}[theorem]{Example}
\begin{descent direction example}
    The descent direction can be $\mathbf{d}=-\nabla f(\mathbf{x})$,
    since as long as $\nabla f(\mathbf{x})\neq 0$ 
    ($\mathbf{x}$ is a non-stationary point), we have
    \[
        f'(\mathbf{x};-\nabla f(\mathbf{x}))=-\nabla f(\mathbf{x})^Tf(\mathbf{x})
        =-\norm{\nabla f(\mathbf{x})}^2 < 0.
    \]
\end{descent direction example}

\newmdtheoremenv[style=defEnv]{descent property}[theorem]{Lemma}
\begin{descent property}
    Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a continuously differentiable function.
    Let $\mathbf{x}\in\mathbb{R}^{n}$. 
    Suppose that $\mathbf{d}$ is a descent direction of $f$ at $\mathbf{x}$,
    then
    \[
        \exists\epsilon>0\text{ s.t. }\forall
        t\in(0,\epsilon],f(\mathbf{x}+t\mathbf{d})<f(\mathbf{x}).
    \]
\end{descent property}

\newmdtheoremenv[style=defEnv]{descent optimal direction}[theorem]{Lemma}
\begin{descent optimal direction}
    Let $f$ be a continuously differentiable function and
    $\mathbf{x}\in\mathbb{R}^{n}$ be a non-stationary point ($\nabla
    f(\mathbf{x})\neq 0$), then the optimal solution of
    \[
        \underset{\mathbf{d}}{\text{min}}\left\{f'(\mathbf{x};\mathbf{d}:\norm{\mathbf{d}}=1\right\}
    \]
    is $\mathbf{d}=-\frac{\nabla f(\mathbf{x})}{\norm{\nabla f(\mathbf{x})}}$.
\end{descent optimal direction}

\newmdtheoremenv[style=defEnv]{zig-zag effect}[theorem]{Lemma}
\begin{zig-zag effect}
    Let ${\left\{\mathbf{x}^k\right\}}_{k>0}$ be the sequence generated by the
    gradient descent method with \emph{exact} line search for solving a problem
    of minimizing a continuously differentiable function $f$. Then $\forall
    k=0,1,2,\ldots$,
    \[
        {(\mathbf{x}^{k+2}-\mathbf{x}^{k+1})}^{T}
        (\mathbf{x}^{k+1}-\mathbf{x}^k)=0.
    \]
\end{zig-zag effect}


\section{Stepsize Selection Rules}

Finding the right $t^k\in\mathbb{R}^{n}$, called the \textbf{stepsize}, is
referred in the literature as \textbf{line search}.

\begin{enumerate}
    \item Constant stepsize: $t^k=\overline{t}$ $\forall k$.
    \item Exact stepsize: $t^{k}$ is a minimizer of $f$ along the ray
        $\mathbf{x}^{k}_t\mathbf{d}^{k}$:
        \[
            t^{k}\in \underset{t\ge
            0}{\text{argmin}}f(\mathbf{x}^{k}+t\mathbf{d}^{k})
        \]
    \item Backtracking (Armijo rule): let $s>0,\alpha\in(0,1),\beta\in(0,1)$,
        and initial stepsize $t^k=s$, while
        \[
            f(\mathbf{x}^k)-f(\mathbf{x}^k+t^k\mathbf{d}^k)<-\alpha t^k\nabla
            f(\mathbf{x}^k)^T\mathbf{d}^k
        \]
        set $t^k:=\beta t^k$, iterating until achieving the \textbf{sufficient
        decrease property}
        \[
            f(\mathbf{x}^k)-f(\mathbf{x}^k+t^k\mathbf{d}^k)\ge -\alpha t^k\nabla
            f(\mathbf{x}^k)^T\mathbf{d}^k.
        \]
\end{enumerate} 

\section{Convergence}

\newmdtheoremenv[style=defEnv]{lipschitz gradient}[theorem]{Definition}
\begin{lipschitz gradient}
    Let $f$ be a continuously differentiable function over $\mathbb{R}^{n}$.
    We say that $f$ has a \textbf{Lipschitz gradient} if
    \[
        \exists L\ge 0\text{ s.t.
        }\forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{n},\;
        \norm{\nabla f(\mathbf{x})-\nabla f(\mathbf{y})}\le
        L\norm{\mathbf{x}-\mathbf{y}}.
    \]
    $L$ is called the \textbf{Lipschitz constant}.
\end{lipschitz gradient}

\underline{\textbf{Comments}}:
\begin{itemize}
    \item The class of functions with Lipschitz gradient with constant $L$ 
        is denoted as $C_L^{1,1}(\mathbb{R}^{n})$ or just $C_L^{1,1}$.
        When $L$ is irrelevant, we simply denote the class by $C^{1,1}$.
    \item If $\nabla f$ is Lipschitz with constant $L$, then it is also
        Lipschitz with constant $L'$ $\forall L'\ge L$.
    \item \underline{Linear functions}: Given $a\in\mathbb{R}^{n}$, the function
        $f(\mathbf{x})=a^T\mathbf{x}$ is in $C_0^{1,1}$.
    \item \underline{Quadratic functions}: Let $A\in\mathbb{R}^{n\times n}$,
        $\mathbf{b}\in\mathbb{R}^{n}$, and $c\in\mathbb{R}$, then the function
        $f(\mathbf{x})=\mathbf{x}^TA\mathbf{x}+2\mathbf{b}^T\mathbf{x}+c$ is
        $C_{2\;\norm{A}_2}^{1,1}$.
\end{itemize} 

\newmdtheoremenv[style=defEnv]{equivalence to boundedness of hessian}[theorem]{Theorem}
\begin{equivalence to boundedness of hessian}
    Let $f$ be a continuously differentiable function over $\mathbb{R}^{n}$.
    Then
    \[
        f\in C_L^{1,1}(\mathbb{R}^{n})
        \iff
        \norm{\nabla^2f(\mathbf{x})}\le L\;\forall \mathbf{x}\in\mathbb{R}^{n}.
    \]
\end{equivalence to boundedness of hessian}

\newmdtheoremenv[style=defEnv]{sufficient decrease of gradient method}[theorem]
{(Sufficient decrease of the gradient method) Lemma}
\begin{sufficient decrease of gradient method}
    Let $f\in C_L^{1,1}(\mathbb{R}^{n})$.
    Let ${\{\mathbf{x}^k\}}_{k\ge 0}$ be the sequence generated by the gradient
    method for solving 
    \[
        \underset{\mathbf{x}\in\mathbb{R}^{n}}{\min}f(\mathbf{x})
    \]
    with one of the following stepsize strategies:
    \begin{itemize}
        \item constant stepsize $\overline{t}\in\left(0,\frac{2}{L}\right)$,
        \item exact line search
        \item backtracking procedure with parameters $s\in\mathbb{R}_{++}$,
            $\alpha\in(0,1)$, and $\beta\in(0,1)$,
    \end{itemize} 
    then
    \[
        f(\mathbf{x}^k)-f(\mathbf{x}^{k+1})\ge M\norm{\nabla f(\mathbf{x}^k)}^2
    \]
    where
    \[
        M=
        \begin{cases}
            \overline{t}\left(1-\frac{\overline{t}L}{2}\right) & \text{constant stepsize} \\
            \frac{1}{2L} & \text{exact line search} \\
            \alpha\min{\left\{s,\frac{2(1-\alpha)\beta}{L}\right\}} & \text{backtracking}
        \end{cases} 
    \]
\end{sufficient decrease of gradient method}

\newmdtheoremenv[style=defEnv]{convergence of the gradient method}[theorem]
{(Convergence of the gradient method) Theorem}
\begin{convergence of the gradient method}
    Let $f\in C_L^{1,1}(\mathbb{R}^{n})$ and is bounded below over
    $\mathbb{R}^{n}$.
    Let ${\{\mathbf{x}^k\}}_{k\ge 0}$ be the sequence generated by the gradient
    method for solving 
    \[
        \underset{\mathbf{x}\in\mathbb{R}^{n}}{\min}f(\mathbf{x})
    \]
    with one of the following stepsize strategies:
    \begin{itemize}
        \item constant stepsize $\overline{t}\in\left(0,\frac{2}{L}\right)$,
        \item exact line search
        \item backtracking procedure with parameters $s\in\mathbb{R}_{++}$,
            $\alpha\in(0,1)$, and $\beta\in(0,1)$,
    \end{itemize} 
    then
    \begin{enumerate}
        \item $\forall k, f(\mathbf{x}^{k+1})<f(\mathbf{x}^k)$ unless $\nabla
            f(\mathbf{x}^k)=0$.
        \item $\nabla f(\mathbf{x}^k)\rightarrow 0$ as $k\rightarrow\infty$.
    \end{enumerate} 
\end{convergence of the gradient method}

\section{Condition Number and Convergence for Quadratic Function}

\newmdtheoremenv[style=defEnv]{condition number}[theorem]{Definition}
\begin{condition number}
    Let $A\in\mathbb{R}^{n\times n}$ be positive definite, Then the
    \textbf{condition number} of $A$ is
    \[
        \kappa(A)=\frac{\lambda_\text{max}(A)}{\lambda_\text{min}(A)}
    \]
    where $\lambda_\text{max}(A)$ and $\lambda_\text{min}(A)$ are the largest
    and smallest eigenvalues respectively.
\end{condition number}

\newmdtheoremenv[style=defEnv]{kantorovich inequality}[theorem]{(Kantorovich inequality)Lemma}
\begin{kantorovich inequality}
    Let $A\in\mathbb{R}^{n\times n}$ be positive definite. Then
    \[
        \forall \mathbf{0}\neq\mathbf{x}\in\mathbb{R}^{n},\;
        \frac{{(\mathbf{x}^T\mathbf{x})}^{2}}{(\mathbf{x}^TA\mathbf{x})(\mathbf{x}^TA^{-1}\mathbf{x})}
        \ge\frac{4\lambda_\text{max}(A)\lambda_\text{min}(A)}{{(\lambda_\text{max}(A)+\lambda_\text{min}(A))}^{2}}.
    \]
\end{kantorovich inequality}

\newmdtheoremenv[style=defEnv]{gradient method for minimizing quadratic function}[theorem]
{(Convergence for quadratic function) Theorem}
\begin{gradient method for minimizing quadratic function}
    Let ${\{\mathbf{x}^k\}}_{k\ge 0}$ be the sequence generated by the gradient
    method for solving 
    \[
        \underset{\mathbf{x}\in\mathbb{R}^{n}}{\min}f(\mathbf{x}^TA\mathbf{x})\quad
        (A\succ 0),
    \]
    then $\forall k=0,1,\ldots,$
    \[
        f(\mathbf{x}^{k+1})\le{\left(\frac{\lambda_\text{max}(A)-\lambda_\text{min}(A)}
        {\lambda_\text{max}(A)+\lambda_\text{min}(A)}\right)}^{2}f(\mathbf{x}^k)
        ={\left(\frac{\kappa(A)-1}{\kappa(A)+1}\right)}^{2}f(\mathbf{x}^k).
    \]
\end{gradient method for minimizing quadratic function}

\section{Scaled Gradient Method}

A way to mitigate the slow convergence due to poor conditioning of the Hessian
is to formulate a rescaled version of the problem. From the minimization problem
\[
    \min\left\{f(\mathbf{x}):\mathbf{x}\in\mathbb{R}^{n}\right\}
\]
we introduce a nonsingular matrix $S\in\mathbb{R}^{n\times n}$ to make the
linear change of variables $\mathbf{x}=S\mathbf{y}$ and obtain the equivalent
problem
\[
    \min\left\{g(\mathbf{y})\equiv f(S\mathbf{y}):\mathbf{y}\in\mathbb{R}^{n}\right\}
\]
Since $\nabla g(\mathbf{y})=S^T\nabla f(S\mathbf{y})=S^T\nabla f(\mathbf{x})$,
the gradient method for the rescaled problem reads
\[
    \mathbf{y}^{k+1}=\mathbf{y}^{k}-t^kS^T\nabla f(S\mathbf{y}^k).
\]
Multiplying both sides by $S$, with $\mathbf{x}^k=S\mathbf{y}^k$, and define
$D=SS^T$, we have
\[
    \mathbf{x}^{k+1}=\mathbf{x}^{k}-t^kD\nabla f(\mathbf{x}^k).
\]
Since $D\succ 0$, so
\[
    f'(\mathbf{x}^k;-D\nabla f(\mathbf{x}^k))=-\nabla f(\mathbf{x}^k)^TD\nabla
    f(\mathbf{x}^k)<0.
\]
A well-known choice for $D^k$ is to pick $D^k={(\nabla^2f(\mathbf{x}^k))}^{-1}$
(Newton's method). Another alternative is to use a diagonal scaling, e.g.
\[
    {\left(D^k\right)}_{ii}={\left(\frac{\partial^2f(\mathbf{x}^k)}{\partial
    x_i^2}\right)}^{-1}
\]

\section{The Kaczmarz Algorithm}

The \emph{Kaczmarz Algorithm} solves the linear system
\[
    A\mathbf{x}=\mathbf{b}
\]
by iterating projections along the $i$-th row of the matrix $A$, denoted by
$\mathbf{a}_i^T$:
\[
    \mathbf{x}^{k+1}=\mathbf{x}^k+\frac{b_i-\mathbf{a}_i^T\mathbf{x}^k}{\norm{\mathbf{a}_i}^2}\mathbf{a}_i
\]
In the original Kaczmarz algorithm, the $i$-th row is chosen periodically by
cycling through all rows. If chooses $i$-th row randomly, we can show that the
algorithm converges exponentially, and this is known as \emph{randomized Kaczmarz Algorithm}.

\medskip\noindent
The algorithm works because the problem of solving the linear system 
$A\mathbf{x}=\mathbf{b}$ could be formulated 
as an optimization problem
\[
    \underset{\mathbf{x}}{\text{min}}\frac{1}{2m}\norm{A\mathbf{x}-\mathbf{b}}^2
    =\frac{1}{2m}\sum_{i=1}^{m} {(\mathbf{a}_i^T\mathbf{x}-\mathbf{b}_i)}^{2}
\]
for which the gradient descent method could be constructed as
\[
    \mathbf{x}^{k+1}=\mathbf{x}^k-\frac{t}{m}A^T(A\mathbf{x}-\mathbf{b})
\]
but the problem could also be formulated as
\[
    \underset{\mathbf{x}}{\text{min}}\frac{1}{2m}\norm{A\mathbf{x}-\mathbf{b}}^2
    =\frac{1}{2m}\sum_{i=1}^{m} {(\mathbf{a}_i^T\mathbf{x}-b_i)}^{2}
    =\frac{1}{2}\mathbb{E}_i[\mathbf{a}_i^T\mathbf{x}-b_i],
\]
which can then be translated to the action of randomly picking a row of $A$,
becoming
\[
    \mathbf{x}^{k+1}=\mathbf{x}^k-\frac{t}{m}(\mathbf{a}_i^T\mathbf{x}-b_i)\mathbf{a}_i
\]

\section{Stochastic Gradient Descent}

\newmdtheoremenv[style=defEnv]{convergence of SGD}[theorem]{Theorem}
\begin{convergence of SGD}
    Assuming that
    \begin{itemize}
        \item The cost $g(\mathbf{x})$ is such that
            \[
                \norm{\nabla g(\mathbf{x})-\nabla g(\mathbf{y})}\le
                L\norm{\mathbf{x}-\mathbf{y}},\quad
                \text{and}\quad
                \nabla^2g(\mathbf{x})\succeq\mu I.
            \]
        \item The sample gradient $\nabla Q_i(\mathbf{x}^k)$ is an unbiased
            estimate of $\nabla g(\mathbf{x}^k)$.
        \item \[
                \forall \mathbf{x},
                \mathbb{E}_i\left[\norm{Q_i(\mathbf{x})}^2\right]\le
                \sigma^2+c\norm{\nabla g(\mathbf{x})}^2.
        \]
    \end{itemize} 
    Then if $t^k\equiv t\le \frac{1}{Lc}$, then SGD achieves
    \[
        \mathbb{E}\left[g(\mathbf{x}^k)-g(\mathbf{x}^*)\right]
        \le\frac{tL\sigma^2}{2\mu}+{(1-t\mu)}^{k}(g(\mathbf{x}^0)-g(\mathbf{x}^*)).
    \]
\end{convergence of SGD}
\underline{\textbf{Comments}}
\begin{enumerate}
    \item Fast (linear) convergence during the first iterations.
    \item Convergence to a neighbourhood of $\mathbf{x}^*$, without further
        progress.
    \item If gradient computation is noiseless ($\sigma=0$), then linear
        convergence to optimal point.
    \item A smaller stepsize $t$ yield better converging points.
\end{enumerate} 

\newmdtheoremenv[style=defEnv]{batch gradient descent}[theorem]{Definition}
\begin{batch gradient descent}
    The \textbf{batch gradient descent} algorithm is defined as
    \[
        \mathbf{x}^{k+1}=\mathbf{x}^k-t^k\nabla g(\mathbf{x}^k)
        =\mathbf{x}^k-\frac{t^k}{|K|}\sum_{i\in K}\nabla Q_i(\mathbf{x}^k),
    \]
    where $K$ denotes a set of $p$ randomly selected datapoints.
\end{batch gradient descent}


\chapter{Convexity}

\section{Convex Sets}

\newmdtheoremenv[style=defEnv]{convex sets}[theorem]{Definition}
\begin{convex sets}
    A set $C\subseteq\mathbb{R}^{n}$ is called \textbf{convex} if
    \[
        \forall \mathbf{x},\mathbf{y}\in C\text{ and }\lambda\in[0,1],
        \lambda\mathbf{x}+(1-\lambda)\mathbf{y}\in C.
    \]
    Equivalently, for any $\mathbf{x},\mathbf{y}\in C$, the line segment
    $[\mathbf{x},\mathbf{y}]$ is also in $C$.
\end{convex sets}

\newtheorem{convex sets example}[theorem]{Example}
\begin{convex sets example}
    Very important convex sets
    \begin{itemize}
        \item A line in $\mathbb{R}^{n}$ is a set of the form
            \[
                L=\left\{\mathbf{z}+t\mathbf{d}:t\in\mathbb{R}\right\},
            \]
            where $\mathbf{z},\mathbf{d}\in\mathbb{R}^{n}$ and $\mathbf{d}\neq \mathbf{0}$.
        \item $[\mathbf{x},\mathbf{y}],(\mathbf{x},\mathbf{y})$ for
            $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}(\mathbf{x}\neq\mathbf{y})$,
            $\emptyset$, and $\mathbb{R}^{n}$.
        \item A \textbf{hyperplane} is a set of the form
            \[
                H=\left\{\mathbf{x}\in\mathbb{R}^{n}:\mathbf{a}^T\mathbf{x}=b\right\}\quad
                (\mathbf{a}\in\mathbb{R}\backslash\left\{\mathbf{0}\right\},b\in\mathbb{R})
            \]
        \item The associated \textbf{half space} is the set
            \[
                H^-=\left\{\mathbf{x}\in\mathbb{R}^{n}:\mathbf{a}^T\mathbf{x}\le b\right\}.
            \]
        \item The open ball $B(\mathbf{c},r)$ and the closed ball $B[\mathbf{c},r]$.
        \item The \textbf{ellipsoid} is a set of the form
            \[
                E=\left\{\mathbf{x}\in\mathbb{R}^{n}:\mathbf{x}^TQ\mathbf{x}+2\mathbf{b}^T\mathbf{x}+c\le 0\right\}
            \]
            where $Q\in\mathbb{R}^{n\times n}$ is positive semidefinite,
            $\mathbf{b}\in\mathbb{R}^{n}$ and $c\in\mathbb{R}$.
    \end{itemize} 
\end{convex sets example}

\newmdtheoremenv[style=defEnv]{intersection of convex sets}[theorem]{Lemma}
\begin{intersection of convex sets}
    Let $C_i\subseteq\mathbb{R}^{n}$ be a convex set for any $i\in I$, where $I$
    is an index set (possibly infinite), then $\bigcap_{i\in I}C_i$ is convex.
\end{intersection of convex sets}
\underline{\textbf{Comments}}: A direct consequence of the above is that convex
polytopes of the form
\[
    P=\left(\mathbf{x}\in\mathbb{R}^{n}:A\mathbf{x}\le\mathbf{b}\right),
\]
are convex since they are generated as the intersection of $m$ half-spaces
$\mathbf{a}_i^T\mathbf{x}\le b_i$.

\newmdtheoremenv[style=defEnv]{convex sets properties}[theorem]{Theorem}
\begin{convex sets properties}
    Several important algebraic properties of convex sets:
    \begin{enumerate}
        \item Let $C_1,C_2,\ldots,C_k\subseteq\mathbb{R}^{n}$ be convex sets and
            let $\mu_1,\mu_2,\ldots,\mu_k\in\mathbb{R}$, then the set
            $\mu_1C_1+\mu_2C_2+\cdots+\mu_kC_k$ is convex.
        \item Let $C_i\subseteq\mathbb{R}^{k_i}$, $i=1,\ldots,m$ be convex sets,
            then the cartesian product
            \[
                C_1\times C_2\times\cdots\times C_m
                =\left\{(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_m):\mathbf{x}_i\in
                C_i,i=1,2,\ldots,m\right\}
            \]
            is convex.
        \item Let $M\subseteq\mathbb{R}^{n}$ be a convex set and let
            $A\in\mathbb{R}^{m\times n}$, then the set
            \[
                A(M)=\left\{A\mathbf{x}:\mathbf{x}\in M\right\}
            \]
            is convex.
        \item Let $D\subseteq\mathbb{R}^{m}$ be convex and let
            $A\in\mathbb{R}^{m\times n}$, then the set
            \[
                A^{-1}(D)=\left\{\mathbf{x}\in\mathbb{R}^{n}:A\mathbf{x}\in D\right\}
            \]
            is convex.
    \end{enumerate} 
\end{convex sets properties}

\section{Convex Hull}

\newmdtheoremenv[style=defEnv]{convex combinations}[theorem]{Definition}
\begin{convex combinations}
    Given $m$ points
    $\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_m\in\mathbb{R}^{n}$, a
    \textbf{convex combination} of these $m$ points is a vector of the form 
    \[
        \lambda_1\mathbf{x}_1+\lambda_2\mathbf{x}_2+\cdots+\lambda_m\mathbf{x}_m
    \]
    where $\lambda_i\in\mathbb{R}_+$ for $i=1,2,\ldots,m$ and satisfy
    $\sum_{i=1}^m\lambda_i=1$ ($\pmb{\lambda}\in\Delta_m$).
\end{convex combinations}

\newmdtheoremenv[style=defEnv]{convex combination is convex}[theorem]{Theorem}
\begin{convex combination is convex}
    Let $C\subseteq\mathbb{R}^{n}$ be a convex set and let $\mathbf{x}_i\in C$
    for $i=1,2,\ldots,m$. Then for any $\pmb{\lambda}\in\Delta_m$, the relation
    \[
        \sum_{i=1}^{m} \lambda_i \mathbf{x}_i\in C
    \]
    holds.
\end{convex combination is convex}


\newmdtheoremenv[style=defEnv]{convex hull}[theorem]{Definition}
\begin{convex hull}
    Let $S\subseteq\mathbb{R}^{n}$. The \textbf{convex hull} of $S$, denoted by
    conv($S$), is the set comprising all the convex combinations of vectors from
    $S$:
    \[
        \text{conv}(S)=\left\{\sum_{i=1}^{k} \lambda_i\mathbf{x}_i:
        \mathbf{x}_i,\mathbf{x}_2,\ldots,\mathbf{x}_k\in S,\pmb{\lambda}\in\Delta_k\right\}
    \]
\end{convex hull}
\underline{\textbf{Comment}}: conv($S$) is the ``smallest'' convex set
containing $S$.

\newmdtheoremenv[style=defEnv]{caratheodory}[theorem]{Theorem}
\begin{caratheodory}
    Let $S\subseteq\mathbb{R}^{n}$ and let $\mathbf{x}\in\text{conv}(S)$. Then
    \[
        \exists\pmb{\lambda}\in\Delta_{n+1},
        \exists\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{n+1}\in S
        \text{ s.t. }
        \mathbf{x}=\sum_{i=1}^{n+1} \lambda_i\mathbf{x}_i.
    \]
\end{caratheodory}

\newtheorem{caratheodory example}[theorem]{Example}
\begin{caratheodory example}
    For $n=2$, consider the four vectors
    \[
        \mathbf{x}_1=
        \begin{pmatrix}
            1 \\
            1
        \end{pmatrix},
        \mathbf{x}_2=
        \begin{pmatrix}
            1 \\
            2
        \end{pmatrix},
        \mathbf{x}_3=
        \begin{pmatrix}
            2 \\
            1
        \end{pmatrix},
        \mathbf{x}_4=
        \begin{pmatrix}
            2 \\
            2
        \end{pmatrix},
    \]
    and let
    $\mathbf{x}\in\text{conv}(\left\{\mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3,\mathbf{x}_4\right\})$
    be given by
    \[
        \mathbf{x}=\frac{1}{8}\mathbf{x}_1+\frac{1}{4}\mathbf{x}_2+\frac{1}{2}\mathbf{x}_3+\frac{1}{8}\mathbf{x}_4
        =
        \begin{pmatrix}
            \dfrac{13}{8} \\[2ex]
            \dfrac{11}{8}
        \end{pmatrix}
        \quad\Longrightarrow\quad
        \pmb{\lambda}=
        \begin{pmatrix}
            1/8 \\
            1/4 \\
            1/2 \\
            1/8
        \end{pmatrix},
    \]
    We can find out that
    \[
        (\mathbf{x}_2-\mathbf{x}_1)
        +(\mathbf{x}_3-\mathbf{x}_1)
        -(\mathbf{x}_4-\mathbf{x}_1)=0
        \quad\Longrightarrow\quad
        \pmb{\mu}=
        \begin{pmatrix}
            -1 \\
            1 \\
            1 \\
            -1
        \end{pmatrix}.
    \]
    Since we need to satisfy that $\forall i\in\left\{1,2,3,4\right\}$,
    $\lambda_i+\alpha\mu_i\ge 0$, we need to compute
    \[
        \epsilon=\underset{i:\mu_i<0}{\min}\left\{-\frac{\lambda_i}{\mu_i}\right\}
    \]
    so that $\lambda_j+\epsilon\mu_j=0$ for
    $j\in\underset{i:\mu_i<0}{\text{argmin}}\left\{-\dfrac{\lambda_i}{\mu_i}\right\}$,
    thereby reducing the number of $\mathbf{x}_i$'s required for expressing
    $\mathbf{x}$. From the four inequalities, we can obtain that
    \[
        \begin{cases}
            \alpha\le 1/8 \\
            \alpha\ge -1/4 \\
            \alpha\ge -1/2 \\
            \alpha\le 1/8 \\
        \end{cases} 
    \]
    and $\epsilon=\frac{1}{8}$. Substituting $\alpha=\epsilon$, we can obtain
    that
    \[
        \mathbf{x}=\frac{3}{8}\mathbf{x}_2+\frac{5}{8}\mathbf{x}_3.
    \]
\end{caratheodory example}


\newmdtheoremenv[style=defEnv]{extreme points}[theorem]{Definition}
\begin{extreme points}
    Let $S\subseteq\mathbb{R}^{n}$ be a convex set. A point $\mathbf{x}\in S$ is
    called an \textbf{extreme point} of $S$ if
    $\nexists\mathbf{x}_1,\mathbf{x}_2\in S(\mathbf{x}_1\neq\mathbf{x}_2$
    and $\lambda\in(0,1)$, s.t.
    $\mathbf{x}=\lambda\mathbf{x}_1+(1-\lambda)\mathbf{x}_2$.
    The set of extreme point is denoted by ext($S$).
\end{extreme points}

\newmdtheoremenv[style=defEnv]{krein-milman}[theorem]{Theorem}
\begin{krein-milman}
    Let $S\subseteq\mathbb{R}^{n}$ be a compact convex set.
    Then
    \[
        S=\text{conv}(\text{ext}(S)).
    \]
\end{krein-milman}

\section{Convex Functions}

\newmdtheoremenv[style=defEnv]{convex function}[theorem]{Definition}
\begin{convex function}
    A function $f:C\rightarrow\mathbb{R}$ defined on a convex set
    $C\subseteq\mathbb{R}^{n}$ is called \textbf{convex} (or convex over $C$) if
    \[
        \forall\mathbf{x},\mathbf{y}\in C,\lambda\in[0,1],
        f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})\le
        \lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y})
    \]
\end{convex function}

\newmdtheoremenv[style=defEnv]{strict convex function}[theorem]{Definition}
\begin{strict convex function}
    A function $f:C\rightarrow\mathbb{R}$ defined on a convex set
    $C\subseteq\mathbb{R}^{n}$ is called \textbf{strict convex} if
    \[
        \forall\mathbf{x}\neq\mathbf{y}\in C,\lambda\in(0,1),
        f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})<
        \lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y})
    \]
\end{strict convex function}

\newmdtheoremenv[style=defEnv]{concavity}[theorem]{Definition}
\begin{concavity}
    A function is called \textbf{concave} if $-f$ is convex. Similarly, $f$ is
    called \textbf{strictly concave} if $-f$ is strictly convex.
\end{concavity}

\newtheorem{convex function example}[theorem]{Example}
\begin{convex function example}
    Several examples of convex functions:
    \begin{itemize}
        \item Affine functions: $f(\mathbf{x})=a^T\mathbf{x}+b$, where
            $a\in\mathbb{R}^n$ and $b\in\mathbb{R}$. Take
            $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$ and $\lambda\in[0,1]$, then
            \[
                f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})=
                \lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y}).
            \]
        \item Norms: $g(\mathbf{x})=\norm{\mathbf{x}}$. Take
            $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$ and $\lambda\in[0,1]$, then
            \[
                g(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})\le
                \norm{\lambda\mathbf{x}}+\norm{(1-\lambda)\mathbf{y}}=
                \lambda g(\mathbf{x})+(1-\lambda)g(\mathbf{y})
            \]
    \end{itemize} 
\end{convex function example}

\newmdtheoremenv[style=defEnv]{jensen's inequality}[theorem]{(Jensen's Inequality) Theorem}
\begin{jensen's inequality}
    Let $f:C\rightarrow\mathbb{R}$ be a convex function where
    $C\subseteq\mathbb{R}^{n}$ is a convex set. Then $\forall
    \mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_k\in C$ and
    $\pmb{\lambda}\in\Delta_k$,
    \[
        f\left(\sum_{i=1}^{k} \lambda_i\mathbf{x}_i\right)\le
        \sum_{i=1}^{k} \lambda_i f(\mathbf{x}_i).
    \]
\end{jensen's inequality}


\section{First-order Characterization of Convex Functions}

\newmdtheoremenv[style=defEnv]{gradient inequality}[theorem]{Theorem}
\begin{gradient inequality}
    Let $f:C\rightarrow\mathbb{R}$ be a continuously differentiable function
    defined on a convex set $C\subseteq\mathbb{R}^{n}$. Then 
    \[
        f \text{ is convex over } C
        \quad\iff\quad
        \forall\mathbf{x},\mathbf{y}\in C,
        f(\mathbf{x})+\nabla f(\mathbf{x})^T(\mathbf{y}-\mathbf{x})\le
        f(\mathbf{y})
    \]
    An analogous result holds for strictly convex functions with a strict
    inequality.
\end{gradient inequality}
\underline{\textbf{Comment}}: For a convex function $f$ defined on
$\mathbb{R}^{2}$, the tangent plane at
every point is always below $f$.

\newmdtheoremenv[style=defEnv]{stationarity implies global optimality}[theorem]
{(Global optimality test for convex(concave) function) Theorem}
\begin{stationarity implies global optimality}
    Let $f$ be a continuously differentiable function which is
    \underline{convex} over a
    convex set $C\subseteq\mathbb{R}^{n}$. 
    Then
    \[
        \nabla f(\mathbf{x}^*)=0\text{ for some }\mathbf{x}^*\in C
        \quad\Longrightarrow\quad
        \mathbf{x}^*\text{ is the global \underline{minimizer} of $f$ over $C$.}
    \]
    This is the same for concave function being related to global maximizer.
\end{stationarity implies global optimality}

\newmdtheoremenv[style=defEnv]{convexity of quadratic function}[theorem]
{(Convexity of quadratic function) Theorem}
\begin{convexity of quadratic function}
    Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be the quadratic function given
    by $f(\mathbf{x})=\mathbf{x}^TA\mathbf{x}+2\mathbf{b}^T\mathbf{x}+c$ where
    $A\in\mathbb{R}^{n\times n}$ is symmetric, $b\in\mathbb{R}^{n}$, and
    $c\in\mathbb{R}$. Then
    \[
        f\text{ is (strictly) convex}
        \quad\iff\quad
        A\succeq 0(A\succ 0).
    \]
\end{convexity of quadratic function}

\newmdtheoremenv[style=defEnv]{monotonicity of the gradient}[theorem]
{(Monotonicity of the gradient) Theorem}
\begin{monotonicity of the gradient}
    Suppose that $f$ is a continuously differentiable function over a convex set
    $C\subseteq\mathbb{R}^{n}$, then
    \[
        f\text{ is convex over }C
        \quad\iff\quad
        \forall\mathbf{x},\mathbf{y}\in C,
        {(\nabla f(\mathbf{x})-\nabla
        f(\mathbf{y}))}^T(\mathbf{x}-\mathbf{y})\ge 0.
    \]
    An analogous result holds for strictly convex functions with a strict
    inequality.
\end{monotonicity of the gradient}

\begin{proof}
    If $f$ is convex, then
    \[
        f(y)\ge f(x)+\nabla f(x)\cdot(y-x)
    \]
    and
    \[
        f(x)\ge f(y)+\nabla f(y)\cdot(x-y)
    \]
    so that by adding the above inequalities, we obtain the result.
\end{proof} 

\section{Second-order Characterization of Convex Functions}

\newmdtheoremenv[style=defEnv]{second-order characterization of convexity}[theorem]{Theorem}
\begin{second-order characterization of convexity}
    Let $f$ be a twice continuously differentiable function over an open convex
    set $C\subseteq\mathbb{R}^{n}$. Then
    \[
        f\text{ is convex over }C
        \quad\iff\quad
        \forall\mathbf{x}\in C,
        \nabla^2f(\mathbf{x})\succeq 0
    \]
\end{second-order characterization of convexity}

\newtheorem{second-order characterization of convexity eg}[theorem]{Example}
\begin{second-order characterization of convexity eg}
    Convexity of the log-sum-exp function
    \[
        f(\mathbf{x})=\log{(e^{x_1}+e^{x_2}+\cdots+e^{x_n})},\;\mathbf{x}\in\mathbb{R}^{n}.
    \]
    The gradient is given by
    \[
        \frac{\partial f}{\partial
        x_i}(\mathbf{x})=\frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}},
        \quad i=1,2,\ldots,n.
    \]
    Therefore, the Hessian is computed as
    \[
        \frac{\partial^2f}{\partial x_i\partial x_j}(\mathbf{x})=
        \begin{cases}
            -\frac{e^{x_i}e^{x_j}}{{\left(\sum_{j=1}^{n} e^{x_j}\right)}^{2}}
            & i \neq j \\
            -\frac{e^{x_i}e^{x_j}}{{\left(\sum_{j=1}^{n} e^{x_j}\right)}^{2}}
            +\frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
            & i = j
        \end{cases} 
    \]
    We can thus write the Hessian matrix as
    \[
        \nabla^2f(\mathbf{x})=\text{diag}(\mathbf{w})-\mathbf{w}\mathbf{w}^T,
        \quad\text{with}\quad
        \mathbf{w}={\left(\frac{e^{x_i}}{\sum_{j=1}^{n}
        e^{x_j}}\right)}^{n}_{i=1}
        \in\Delta_n.
    \]
    For any $\mathbf{v}\in\mathbb{R}^{n}$,
    \[
        \mathbf{v}^T\nabla^2f(\mathbf{x})\mathbf{v}=
        \sum_{i=1}^{n} w_iv_i^2-{(\mathbf{v}^T\mathbf{w})}^{2}\ge 0,
    \]
    since defining $s_i=\sqrt{w_i}v_i,t_i=\sqrt{w_i}$, we have
    \[
        (\mathbf{v}^T\mathbf{w})^2={(\mathbf{s}^T\mathbf{t})}^{2}
        \le\norm{\mathbf{s}}^2\norm{\mathbf{t}}^2
        =\left(\sum_{i=1}^{n} w_iv_i^2\right)\left(\sum_{i=1}^{n} w_i\right)
        =\sum_{i=1}^{n} w_iv_i^2.
    \]
    Thus $\nabla^2f(\mathbf{x})\succeq 0$ and hence $f$ is convex over
    $\mathbb{R}^{n}$.
\end{second-order characterization of convexity eg}

\section{More Results of Convex Function}

\newmdtheoremenv[style=defEnv]{operations preserving convexity}[theorem]{Theorem}
\begin{operations preserving convexity}
    Let $f,f_1,f_2,\ldots,f_p$ be convex functions over a convex set
    $C\subseteq\mathbb{R}^{n}$.
    \begin{itemize}
        \item Let $\alpha\ge 0$, then $\alpha f$ is a convex function over $C$.
        \item The sum function $\sum_{i=1}^{p} f_i$ is convex over $C$.
        \item Let $A\in\mathbb{R}^{n\times m}$ and
            $\mathbf{b}\in\mathbb{R}^{n}$. Then the function
            $g(\mathbf{y})=f(A\mathbf{y}+\mathbf{b})$
            is convex over the convex set
            $D=\left\{\mathbf{y}\in\mathbb{R}^{m}:A\mathbf{y}+\mathbf{b}\in C\right\}$.
        \item Let $g:I\rightarrow\mathbb{R}$ be a nondecreasing
            convex function over the interval $I\subseteq\mathbb{R}$.
            Assume that the image of $C$ under $f$ is contained in $I$:
            $f(C)\subseteq I$, then the composition of $g$ and $f$ defined by
            $h(\mathbf{x})\equiv g(f(\mathbf{x}))$ is convex over $C$.
    \end{itemize} 
\end{operations preserving convexity}

\newmdtheoremenv[style=defEnv]{point-wise maximum of convex functions}[theorem]
{(Point-wise maximum of convex functions) Theorem}
\begin{point-wise maximum of convex functions}
    Let $f_1,f_2,\ldots,f_p:C\rightarrow\mathbb{R}$ be $p$ convex functions over
    the convex set $C\subseteq\mathbb{R}^{n}$, then the maximum function
    \[
        f(\mathbf{x})\equiv
        \underset{i=1,2,\ldots,p}{\max}\left\{f_i(\mathbf{x})\right\}
    \]
    is convex over $C$.
\end{point-wise maximum of convex functions}

\newmdtheoremenv[style=defEnv]{preservation of convexity under partial min}[theorem]{Theorem}
\begin{preservation of convexity under partial min}
    Let $f:C\times D\rightarrow\mathbb{R}$ be a convex function defined over the
    set $C\times D$ where $C\subseteq\mathbb{R}^{m}$ and
    $D\subseteq\mathbb{R}^{n}$ are convex sets. Let
    \[
        g(\mathbf{x})=\underset{\mathbf{y}\in D}{\min}f(\mathbf{x},\mathbf{y}),
        \quad\mathbf{x}\in C
    \]
    where we assume that the minimum is finite. Then $g$ is convex over $C$.
\end{preservation of convexity under partial min}
\newtheorem{partial min example}[theorem]{Example}
\begin{partial min example}
    The distance function from a convex set $d_C(\mathbf{x})\equiv
    \underset{y\in C}{\inf}\norm{\mathbf{x}-\mathbf{y}}$.
\end{partial min example}

\newmdtheoremenv[style=defEnv]{continuity of convex functions}[theorem]{Theorem}
\begin{continuity of convex functions}
    Let $f:C\rightarrow\mathbb{R}$ be a convex function defiend over a convex
    set $C\subseteq\mathbb{R}^{n}$. Let $\mathbf{x}_0\in\text{int}(C)$. Then
    $\exists\epsilon>0,L>0$ s.t. $B[\mathbf{x}_0,\epsilon]\subseteq C$ and
    \[
        \forall \mathbf{x}\in B[\mathbf{x}_0,\epsilon],
        |f(\mathbf{x})-f(\mathbf{x}_0)|\le L\norm{\mathbf{x}-\mathbf{x}_0}.
    \]
\end{continuity of convex functions}

\newmdtheoremenv[style=defEnv]{existence of directional derivative of convex function}[theorem]{Theorem}
\begin{existence of directional derivative of convex function}
    Let $f:C\rightarrow\mathbb{R}$ be a convex function over the convex set
    $C\subseteq\mathbb{R}^{n}$. Let $\mathbf{x}\in\text{int}(C)$. Then
    \[
        \forall \mathbf{d}\neq\mathbf{0}, \exists f'(\mathbf{x};\mathbf{d}).
    \]
\end{existence of directional derivative of convex function}

\newmdtheoremenv[style=defEnv]{no maximum inside the convex set}[theorem]{Theorem}
\begin{no maximum inside the convex set}
    Let $f:C\rightarrow\mathbb{R}$ be convex and non-constant over the nonempty
    convex set $C\subseteq\mathbb{R}^{n}$. Then $f$ does not attain a maximum at
    a point in int($C$).
\end{no maximum inside the convex set}

\newmdtheoremenv[style=defEnv]{maximum of a convex function over a compact convex set}[theorem]{Theorem}
\begin{maximum of a convex function over a compact convex set}
    Let $f:C\rightarrow\mathbb{R}$ be convex over the nonempty convex and
    compact set $C\subseteq\mathbb{R}^{n}$. Then there exists at least one
    maximizer of $f$ over $C$ that is an extreme point of $C$.
\end{maximum of a convex function over a compact convex set}






\end{document}
